{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Scraper eBay Asynchrone avec Jupyter Notebook\n",
    "\n",
    "Ce notebook implémente un scraper eBay asynchrone en Python en utilisant les bibliothèques suivantes :\n",
    "\n",
    "- `aiohttp` et `asyncio` pour les requêtes HTTP asynchrones\n",
    "- `BeautifulSoup` pour le parsing HTML\n",
    "- `fake_useragent` pour la rotation des User-Agent\n",
    "- `csv` pour l'export des résultats\n",
    "- `datetime` et `os` pour la gestion des dates et fichiers\n",
    "\n",
    "Le script effectue les opérations suivantes :\n",
    "- Recherche de produits pour différentes catégories (Laptops, Monitors, Smart Watches, Graphics Cards)\n",
    "- Récupération des liens des produits sur plusieurs pages de recherche\n",
    "- Scraping des détails de chaque produit\n",
    "- Sauvegarde des résultats dans des fichiers CSV (un par catégorie)\n",
    "\n",
    "> **Note :** Dans un environnement Jupyter, comme un event loop est déjà en cours, nous utilisons la bibliothèque `nest_asyncio` pour autoriser l'exécution de boucles asynchrones imbriquées.\n"
   ],
   "id": "95997bf601aa64c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %% [code]\n",
    "# Importation des librairies nécessaires\n",
    "import aiohttp\n",
    "import asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import random\n",
    "from fake_useragent import UserAgent\n",
    "from datetime import datetime\n",
    "import os\n"
   ],
   "id": "de38f1937a6a044f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#\n",
    "#  Définition des fonctions utilitaires\n",
    "La fonction `get_headers()` permet de générer des en-têtes HTTP avec un User-Agent aléatoire pour rendre les requêtes moins facilement détectables par eBay.\n"
   ],
   "id": "69905cbde3bd2efc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %% [code]\n",
    "# Initialisation de fake_useragent\n",
    "ua = UserAgent()\n",
    "\n",
    "def get_headers():\n",
    "    return {\n",
    "        'User-Agent': ua.random,\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Referer': 'https://www.ebay.com/',\n",
    "        'DNT': '1'\n",
    "    }\n"
   ],
   "id": "5d6b0bb44c47e021"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "#  Fonctions de Scraping\n",
    "\n",
    " Nous définissons ici plusieurs fonctions asynchrones pour :\n",
    "\n",
    " - **Scraper les détails d'un produit :** `scrape_product_details()`\n",
    " Cette fonction récupère le titre, le prix et d'autres caractéristiques en fonction de la catégorie.\n",
    "\n",
    " - **Scraper une page de résultats de recherche :** `scrape_search_page()`\n",
    " Elle récupère les URL des produits listés sur la page de recherche.\n",
    "\n",
    " - **scraping plusieurs pages pour chaque catégorie :** `scrape_ebay_search()`\n",
    "   Pour chaque catégorie, cette fonction exécute le scraping sur plusieurs pages puis collecte les détails des produits.\n"
   ],
   "id": "89d5ee27f1c07b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "async def scrape_product_details(session, product_url, category):\n",
    "    try:\n",
    "        # Attendre un délai aléatoire pour simuler un comportement humain\n",
    "        await asyncio.sleep(random.uniform(2, 5))\n",
    "        headers = get_headers()\n",
    "\n",
    "        async with session.get(product_url, headers=headers) as response:\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(await response.text(), 'html.parser')\n",
    "\n",
    "            # Extraction du titre du produit\n",
    "            title = soup.find('h1', class_='x-item-title__mainTitle')\n",
    "            title = title.text.strip() if title else 'N/A'\n",
    "\n",
    "            # Extraction du prix\n",
    "            price = soup.find('div', class_='x-price-primary')\n",
    "            price = price.text.strip() if price else 'N/A'\n",
    "\n",
    "            # Extraction des spécifications\n",
    "            specs = {}\n",
    "            for spec in soup.find_all('div', class_='ux-labels-values__labels'):\n",
    "                key = spec.text.strip()\n",
    "                # Recherche de la valeur associée à la clé\n",
    "                value_tag = spec.find_next('div', class_='ux-labels-values__values')\n",
    "                value = value_tag.text.strip() if value_tag else 'N/A'\n",
    "                specs[key] = value\n",
    "\n",
    "            # Dictionnaire de base avec les informations communes\n",
    "            product_details = {\n",
    "                'Title': title,\n",
    "                'Price': price,\n",
    "                'Collection Date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            }\n",
    "\n",
    "            # Enrichissement des données en fonction de la catégorie\n",
    "            if category == \"Laptops\":\n",
    "                product_details.update({\n",
    "                    'RAM': specs.get('RAM Size', 'N/A'),\n",
    "                    'CPU': specs.get('Processor', 'N/A'),\n",
    "                    'Model': specs.get('Model', 'N/A'),\n",
    "                    'Brand': specs.get('Brand', 'N/A'),\n",
    "                    'GPU': specs.get('GPU', 'N/A'),\n",
    "                    'Screen Size': specs.get('Screen Size', 'N/A'),\n",
    "                    'Storage': specs.get('SSD Capacity', 'N/A'),\n",
    "                })\n",
    "            elif category == \"Monitors\":\n",
    "                product_details.update({\n",
    "                    'Screen Size': specs.get('Screen Size', 'N/A'),\n",
    "                    'Maximum Resolution': specs.get('Resolution', 'N/A'),\n",
    "                    'Aspect Ratio': specs.get('Aspect Ratio', 'N/A'),\n",
    "                    'Refresh Rate': specs.get('Refresh Rate', 'N/A'),\n",
    "                    'Response Time': specs.get('Response Time', 'N/A'),\n",
    "                    'Brand': specs.get('Brand', 'N/A'),\n",
    "                    'Model': specs.get('Model', 'N/A'),\n",
    "                })\n",
    "            elif category == \"Smart Watches\":\n",
    "                product_details.update({\n",
    "                    'Case Size': specs.get('Case Size', 'N/A'),\n",
    "                    'Battery Capacity': specs.get('Battery Capacity', 'N/A'),\n",
    "                    'Brand': specs.get('Brand', 'N/A'),\n",
    "                    'Model': specs.get('Model', 'N/A'),\n",
    "                    'Operating System': specs.get('Operating System', 'N/A'),\n",
    "                    'Storage Capacity': specs.get('Storage Capacity', 'N/A')\n",
    "                })\n",
    "            elif category == \"Graphics Cards\":\n",
    "                product_details.update({\n",
    "                    'Brand': specs.get('Brand', 'N/A'),\n",
    "                    'Memory Size': specs.get('Memory Size', 'N/A'),\n",
    "                    'Memory Type': specs.get('Memory Type', 'N/A'),\n",
    "                    'Chipset/GPU Model': specs.get('Chipset/GPU Model', 'N/A'),\n",
    "                    'Connectors': specs.get('Connectors', 'N/A')\n",
    "                })\n",
    "\n",
    "            print(f\"Successfully scraped {category}: {title[:50]}...\")\n",
    "            return product_details\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {product_url}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "async def scrape_search_page(session, query, page, semaphore, category):\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            base_url = \"https://www.ebay.com/sch/i.html\"\n",
    "            params = {'_nkw': query, '_sacat': 0, '_from': 'R40', '_pgn': page}\n",
    "            headers = get_headers()\n",
    "\n",
    "            async with session.get(base_url, params=params, headers=headers) as response:\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(await response.text(), 'html.parser')\n",
    "\n",
    "                # Récupération de tous les éléments produits de la page\n",
    "                items = soup.find_all('div', class_='s-item__wrapper')\n",
    "                product_urls = [item.find('a', class_='s-item__link')['href']\n",
    "                                for item in items if item.find('a', class_='s-item__link')]\n",
    "\n",
    "                print(f\"Scraped page {page} for {category} ({len(product_urls)} products)\")\n",
    "                return product_urls\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping page {page} for {category}: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "async def scrape_ebay_search(categories, max_pages=1):\n",
    "    all_products = {}\n",
    "    semaphore = asyncio.Semaphore(2)\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for category, query in categories.items():\n",
    "            print(f\"\\n{'=' * 30}\\nStarting {category} scraping\\n{'=' * 30}\")\n",
    "            tasks = [scrape_search_page(session, query, page, semaphore, category)\n",
    "                     for page in range(1, max_pages + 1)]\n",
    "            search_results = await asyncio.gather(*tasks)\n",
    "            product_urls = [url for sublist in search_results for url in sublist]\n",
    "\n",
    "            product_tasks = [scrape_product_details(session, url, category) for url in product_urls]\n",
    "            products = await asyncio.gather(*product_tasks)\n",
    "\n",
    "            all_products[category] = [p for p in products if p]\n",
    "            print(f\"\\n{'=' * 30}\\nCompleted {category} ({len(all_products[category])} items)\\n{'=' * 30}\")\n",
    "\n",
    "    return all_products\n"
   ],
   "id": "6f30a48ba774d67f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Sauvegarde des données dans des fichiers CSV\n",
    "\n",
    " Une fois les données extraites, nous souhaitons les sauvegarder dans des fichiers CSV afin de faciliter leur exploitation ultérieure.\n",
    "\n",
    "Les fonctions suivantes permettent :\n",
    "- de déterminer le numéro de scrape suivant (`get_next_scrape_number`),\n",
    " - de sauvegarder les données dans le dossier correspondant à la catégorie (`save_to_csv`).\n"
   ],
   "id": "fa9c2a9996c8431e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "\n",
   "id": "5829d49a59b67676"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %% [code]\n",
    "def get_next_scrape_number(save_directory, category):\n",
    "    \"\"\"Détermine le prochain numéro de scrape pour un dossier donné.\"\"\"\n",
    "    scrape_number = 1\n",
    "    for filename in os.listdir(save_directory):\n",
    "        if filename.startswith(f\"{category}_\") and filename.endswith(\".csv\"):\n",
    "            try:\n",
    "                # Extraction du numéro de scrape depuis le nom de fichier\n",
    "                current_number = int(filename.split('_scrape')[-1].split('.')[0])\n",
    "                if current_number >= scrape_number:\n",
    "                    scrape_number = current_number + 1\n",
    "            except ValueError:\n",
    "                continue\n",
    "    return scrape_number\n",
    "\n",
    "def save_to_csv(data, category, save_directory, fieldnames):\n",
    "    # Formatage du nom de dossier et du fichier\n",
    "    category_folder = category.lower().replace(' ', '_')\n",
    "    category_filename = category_folder\n",
    "    today_date = datetime.now().strftime('%Y_%m_%d')\n",
    "\n",
    "    # Création du dossier de sauvegarde s'il n'existe pas\n",
    "    category_directory = os.path.join(save_directory, category_folder)\n",
    "    os.makedirs(category_directory, exist_ok=True)\n",
    "\n",
    "    # Détermination du prochain numéro de scrape\n",
    "    scrape_number = get_next_scrape_number(category_directory, category_filename)\n",
    "\n",
    "    # Génération du nom de fichier\n",
    "    filename = os.path.join(category_directory, f\"{category_filename}_{today_date}_scrape{scrape_number}.csv\")\n",
    "\n",
    "    # Sauvegarde des données en CSV\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "    print(f\"Saved {len(data)} {category} items to {filename}\")\n"
   ],
   "id": "fb60e83cd4b29983"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " %% [markdown]\n",
    " ## Fonction principale et exécution du scraping\n",
    "\n",
    " La fonction `main()` orchestre l’ensemble du processus :\n",
    " 1. Pour chaque catégorie (Laptops, Monitors, Smart Watches, Graphics Cards), on scrape les pages de recherche.\n",
    " 2. On récupère les URLs des produits, puis on extrait les détails de chacun.\n",
    " 3. On sauvegarde ensuite les données dans un fichier CSV dédié à chaque catégorie.\n",
    "\n",
    " **Note pour les notebooks Jupyter :**\n",
    " Dans certains cas, un événement loop est déjà en cours. Si vous rencontrez une erreur avec `asyncio.run(main())`, vous pouvez installer et utiliser la librairie `nest_asyncio` de la manière suivante :\n",
    " ```python\n",
    " !pip install nest_asyncio\n",
    " import nest_asyncio\n",
    " nest_asyncio.apply()\n",
    " await main()\n",
    " ```\n"
   ],
   "id": "963895d08ef82127"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %% [code]\n",
    "async def main():\n",
    "    categories = {\n",
    "        \"Laptops\": \"laptop\",\n",
    "        \"Monitors\": \"monitor\",\n",
    "        \"Smart Watches\": \"smart watch\",\n",
    "        \"Graphics Cards\": \"graphics card\"\n",
    "    }\n",
    "\n",
    "    max_pages = 18  # Nombre de pages à scraper par catégorie\n",
    "    save_directory = \"data/raw/ebay\"\n",
    "\n",
    "    print(\"\\nStarting eBay scraping...\")\n",
    "    all_products = await scrape_ebay_search(categories, max_pages)\n",
    "\n",
    "    # Champs (colonnes) attendus pour chaque catégorie\n",
    "    category_fields = {\n",
    "        \"Laptops\": ['Title', 'Price', 'RAM', 'CPU', 'Model', 'Brand', 'GPU', 'Screen Size', 'Storage', 'Collection Date'],\n",
    "        \"Monitors\": ['Title', 'Price', 'Screen Size', 'Maximum Resolution', 'Aspect Ratio', 'Refresh Rate', 'Response Time', 'Brand', 'Model', 'Collection Date'],\n",
    "        \"Smart Watches\": ['Title', 'Price', 'Case Size', 'Battery Capacity', 'Brand', 'Model', 'Operating System', 'Storage Capacity', 'Collection Date'],\n",
    "        \"Graphics Cards\": ['Title', 'Price', 'Brand', 'Memory Size', 'Memory Type', 'Chipset/GPU Model', 'Connectors', 'Collection Date']\n",
    "    }\n",
    "\n",
    "    # Sauvegarde des données pour chaque catégorie\n",
    "    for category, products in all_products.items():\n",
    "        if products:\n",
    "            save_to_csv(products, category, save_directory, category_fields[category])\n",
    "\n"
   ],
   "id": "ac242e5ba7ff137e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
