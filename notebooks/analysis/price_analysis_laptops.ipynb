{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse et Visualisation des Données de Laptops\n",
    "\n",
    "Ce Notebook a pour objectif d'analyser et de visualiser les données relatives aux laptops collectées sur deux plateformes (*eBay* et *Flipkart*).\n",
    "\n",
    "Les étapes réalisées incluent :\n",
    "\n",
    "- **Importation des bibliothèques et configuration du logging** : Manipulation des données, visualisation, gestion des chemins, traitement des expressions régulières et journalisation.\n",
    "- **Configuration des chemins** : Définition des répertoires du projet pour les données nettoyées et les résultats.\n",
    "- **Chargement des données nettoyées** : Lecture et préparation des fichiers CSV des laptops.\n",
    "- **Filtrage des produits cross-plateformes** : Identification des produits présents sur les deux plateformes.\n",
    "- **Analyse et visualisation des différences de prix** : Agrégation des données et génération de graphiques.\n",
    "\n",
    "Chaque fonction utilisée est expliquée en détail dans la suite du Notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation des Librairies et Configuration du Logging\n",
    "\n",
    "Dans cette section, nous importons les bibliothèques nécessaires pour :\n",
    "\n",
    "- La manipulation de données avec **pandas**.\n",
    "- La création de graphiques avec **matplotlib** et **seaborn**.\n",
    "- La gestion des chemins avec **pathlib**.\n",
    "- Le traitement des expressions régulières avec **re**.\n",
    "- La journalisation des opérations avec **logging**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import re\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration des Chemins du Projet\n",
    "\n",
    "Nous définissons ici les chemins d’accès aux différents répertoires du projet :\n",
    "- `PROJECT_ROOT` : Racine du projet.\n",
    "- `CLEANED_DATA_PATH` : Dossier contenant les données nettoyées.\n",
    "- `RESULTS_PATH` : Dossier où seront sauvegardés les résultats (les graphiques générés).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure paths\n",
    "PROJECT_ROOT = Path(__file__).resolve().parents[2]\n",
    "CLEANED_DATA_PATH = PROJECT_ROOT / 'data' / 'cleaned'\n",
    "RESULTS_PATH = PROJECT_ROOT / 'results'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des Données Nettoyées\n",
    "\n",
    "La fonction `load_cleaned_data` charge les fichiers CSV nettoyés pour la catégorie **laptops** provenant des plateformes *eBay* et *Flipkart*.  \n",
    "Les étapes réalisées pour chaque fichier sont les suivantes :\n",
    "- **Standardisation des noms de colonnes** : Conversion en minuscules et remplacement des espaces par des underscores.\n",
    "- **Validation des colonnes requises** : Vérification de la présence des colonnes nécessaires (*ram*, *cpu*, *brand*, *price*, *collection_date*).\n",
    "- **Ajout d’une colonne `platform`** : Identification de la plateforme d’origine.\n",
    "- **Normalisation du CPU et du format de la RAM** :\n",
    "  - Application de `normalize_cpu_model` sur la colonne `cpu`.\n",
    "  - Extraction de la valeur numérique pour la RAM.\n",
    "- **Traitement de la date de collecte** : Conversion de la colonne `collection_date` en date.\n",
    "  \n",
    "Les DataFrames ainsi créés sont ensuite concaténés pour obtenir un DataFrame global.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cleaned_data():\n",
    "    \"\"\"Load cleaned data for laptops from eBay and Flipkart\"\"\"\n",
    "    platforms = ['ebay', 'flipkart']\n",
    "    dfs = []\n",
    "    \n",
    "    required_columns = {'ram', 'cpu', 'brand', 'price', 'collection_date'}\n",
    "    \n",
    "    for platform in platforms:\n",
    "        data_dir = CLEANED_DATA_PATH / platform / 'laptops'\n",
    "        if not data_dir.exists():\n",
    "            logger.warning(f\"Missing data directory: {data_dir}\")\n",
    "            continue\n",
    "            \n",
    "        for file in data_dir.glob('*.csv'):\n",
    "            try:\n",
    "                df = pd.read_csv(file)\n",
    "                df.columns = [col.strip().lower().replace(\" \", \"_\") for col in df.columns]\n",
    "                \n",
    "                # Validate columns\n",
    "                missing_cols = required_columns - set(df.columns)\n",
    "                if missing_cols:\n",
    "                    logger.warning(f\"Missing columns in {file}: {missing_cols}\")\n",
    "                    continue\n",
    "                \n",
    "                # Add platform identifier\n",
    "                df['platform'] = platform\n",
    "                \n",
    "                # Normalize CPU and RAM\n",
    "                df['cpu_model'] = df['cpu'].apply(normalize_cpu_model)\n",
    "                df['ram'] = df['ram'].apply(lambda x: int(re.sub(r\"\\D\", \"\", str(x))) if pd.notnull(x) else None)\n",
    "                df['brand'] = df['brand'].str.lower().str.strip()\n",
    "                \n",
    "                # Handle collection date\n",
    "                df['collection_date'] = pd.to_datetime(\n",
    "                    df['collection_date'], errors='coerce'\n",
    "                ).dt.date\n",
    "                \n",
    "                dfs.append(df)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading {file}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtrage des Produits Cross-plateformes\n",
    "\n",
    "La fonction `filter_products_by_platforms` identifie les produits (laptops) disponibles sur les deux plateformes (*eBay* et *Flipkart*).  \n",
    "Pour ce faire, elle :\n",
    "- Crée un identifiant unique (`product_id`) pour chaque produit en combinant les informations de **brand**, **cpu_model** et **ram**.\n",
    "- Regroupe les produits par `product_id` et identifie ceux présents sur les deux plateformes.\n",
    "  \n",
    "Le DataFrame retourné ne contient que les produits qui apparaissent sur *eBay* et *Flipkart*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_products_by_platforms(df: pd.DataFrame):\n",
    "    \"\"\"Find products with matching RAM, CPU, and Brand across platforms\"\"\"\n",
    "    # Create a unique product identifier\n",
    "    df['product_id'] = df['brand'] + \"|\" + df['cpu_model'].astype(str) + \"|\" + df['ram'].astype(str)\n",
    "    \n",
    "    # Find products present on both platforms\n",
    "    platform_groups = df.groupby('product_id')['platform'].unique()\n",
    "    cross_platform = platform_groups[platform_groups.apply(\n",
    "        lambda x: len(set(x) & {'ebay', 'flipkart'}) >= 2\n",
    "    )].index.tolist()\n",
    "    \n",
    "    return df[df['product_id'].isin(cross_platform)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse et Visualisation des Différences de Prix\n",
    "\n",
    "La fonction `analyze_price_differences` réalise l’analyse des écarts de prix pour les produits identifiés comme étant présents sur les deux plateformes.  \n",
    "Les étapes incluent :\n",
    "- **Agrégation** : Calcul du prix moyen par date de collecte pour chaque produit et par plateforme.\n",
    "- **Visualisation** : Génération d’un graphique en barres pour chaque produit, affichant l’évolution des prix dans le temps.\n",
    "- **Sauvegarde des Graphiques** : Chaque graphique est enregistré dans le dossier `RESULTS_PATH / 'laptops'` avec un nom de fichier dérivé de l’identifiant du produit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_price_differences(filtered_df: pd.DataFrame):\n",
    "    \"\"\"Analyze and visualize price differences for matched products\"\"\"\n",
    "    if filtered_df.empty:\n",
    "        logger.info(\"No cross-platform products found for analysis\")\n",
    "        return\n",
    "    \n",
    "    # Group by product_id and platform\n",
    "    grouped = filtered_df.groupby(['product_id', 'platform', 'collection_date'])['price'].mean().unstack(level='platform')\n",
    "    \n",
    "    # Generate bar plots for each product\n",
    "    for product_id, data in grouped.groupby(level=0):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Plot bar graph\n",
    "        data.plot(kind='bar', figsize=(12, 6))\n",
    "        \n",
    "        # Set title and labels\n",
    "        plt.title(f\"Price Trends for {product_id.replace('|', ' ')}\")\n",
    "        plt.xlabel(\"Collection Date\")\n",
    "        plt.ylabel(\"Price (USD)\")\n",
    "        \n",
    "        # Format x-axis dates\n",
    "        plt.xticks(range(len(data.index)), data.index.get_level_values('collection_date'), rotation=45, ha='right')\n",
    "        \n",
    "        # Add legend and grid\n",
    "        plt.legend(title='Platform')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot\n",
    "        sanitized_title = re.sub(r\"[^\\w\\s]\", \"_\", product_id.replace(\"|\", \"_\"))\n",
    "        file_name = f\"Product_{sanitized_title}.png\"\n",
    "        # Créer le dossier de destination s'il n'existe pas\n",
    "        (RESULTS_PATH / 'laptops').mkdir(exist_ok=True, parents=True)\n",
    "        plt.savefig(RESULTS_PATH / 'laptops' / file_name)\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exécution Principale\n",
    "\n",
    "La section suivante orchestre l'exécution complète du processus :\n",
    "\n",
    "1. **Création des répertoires de résultats** si nécessaire.\n",
    "2. **Chargement et traitement des données** via `load_cleaned_data()`.\n",
    "3. **Filtrage des produits cross-plateformes** avec `filter_products_by_platforms()`.\n",
    "4. **Analyse et visualisation des différences de prix** à l'aide de `analyze_price_differences()`.\n",
    "\n",
    "En cas d'erreur, un message détaillé est loggé.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Créer les dossiers de résultats s'ils n'existent pas\n",
    "    RESULTS_PATH.mkdir(exist_ok=True)\n",
    "    (RESULTS_PATH / 'laptops').mkdir(exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        logger.info(\"Loading and processing laptop data...\")\n",
    "        df = load_cleaned_data()\n",
    "        \n",
    "        if df.empty:\n",
    "            logger.error(\"No cleaned data found. Check data/cleaned directories.\")\n",
    "            exit(1)\n",
    "            \n",
    "        logger.info(f\"Loaded {len(df)} records from cleaned data\")\n",
    "        \n",
    "        filtered_df = filter_products_by_platforms(df)\n",
    "        logger.info(f\"Found {len(filtered_df)} cross-platform product entries\")\n",
    "        \n",
    "        analyze_price_differences(filtered_df)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Critical error: {str(e)}\", exc_info=True)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
