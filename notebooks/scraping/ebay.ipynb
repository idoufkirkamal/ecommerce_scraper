{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1622139f1696c46",
   "metadata": {},
   "source": [
    "# Scraping sur eBay\n",
    "\n",
    "Ce notebook a pour objectif de réaliser le scraping de données sur eBay pour différentes catégories de produits (Laptops, Monitors, Smart Watches, Graphics Cards).\n",
    "Pour ce faire, nous utilisons des requêtes asynchrones avec `aiohttp` et `asyncio` ainsi que `BeautifulSoup` pour parser le contenu HTML.\n",
    "Les données extraites seront ensuite sauvegardées dans des fichiers CSV pour une exploitation ultérieure.\n",
    "\n",
    "**Bibliothèques utilisées :**\n",
    "- `aiohttp`, `asyncio` : Pour effectuer des requêtes HTTP asynchrones.\n",
    "- `BeautifulSoup` (via `bs4`) : Pour parser le HTML.\n",
    "- `csv` : Pour enregistrer les résultats dans des fichiers CSV.\n",
    "- `random` : Pour introduire des délais aléatoires et simuler un comportement humain.\n",
    "- `fake_useragent` : Pour générer des User-Agent aléatoires.\n",
    "- `datetime` : Pour gérer les dates d'extraction.\n",
    "- `os` : Pour la gestion des fichiers et dossiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54629b48768de15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import random\n",
    "from fake_useragent import UserAgent\n",
    "from datetime import datetime\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168d265da558be71",
   "metadata": {},
   "source": [
    "## Définition des Headers\n",
    "\n",
    "La fonction `get_headers()` permet de générer des en-têtes HTTP avec un User-Agent aléatoire.\n",
    "Cela aide à éviter d'être bloqué par eBay en simulant un comportement de navigateur standard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48366749640c3c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation de fake_useragent pour obtenir des User-Agent aléatoires\n",
    "ua = UserAgent()\n",
    "\n",
    "def get_headers():\n",
    "    return {\n",
    "        'User-Agent': ua.random,\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Referer': 'https://www.ebay.com/',\n",
    "        'DNT': '1'\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8ac9057f597d5f",
   "metadata": {},
   "source": [
    "## Scraping des Détails d'un Produit\n",
    "\n",
    "La fonction asynchrone `scrape_product_details` reçoit une session, l'URL du produit et la catégorie du produit.\n",
    "Elle effectue les actions suivantes :\n",
    "- Attente d'un délai aléatoire pour simuler le comportement humain.\n",
    "- Envoi d'une requête HTTP GET avec des headers.\n",
    "- Parsing du contenu HTML avec BeautifulSoup.\n",
    "- Extraction du titre, du prix et des spécifications spécifiques selon la catégorie.\n",
    "- Retourne un dictionnaire contenant les détails du produit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a49a9865c1b0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_product_details(session, product_url, category):\n",
    "    try:\n",
    "        # Attendre un délai aléatoire entre 2 et 5 secondes\n",
    "        await asyncio.sleep(random.uniform(2, 5))\n",
    "        headers = get_headers()\n",
    "\n",
    "        async with session.get(product_url, headers=headers) as response:\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(await response.text(), 'html.parser')\n",
    "\n",
    "            # Extraction du titre du produit\n",
    "            title = soup.find('h1', class_='x-item-title__mainTitle')\n",
    "            title = title.text.strip() if title else 'N/A'\n",
    "\n",
    "            # Extraction du prix\n",
    "            price = soup.find('div', class_='x-price-primary')\n",
    "            price = price.text.strip() if price else 'N/A'\n",
    "\n",
    "            # Extraction des spécifications\n",
    "            specs = {}\n",
    "            for spec in soup.find_all('div', class_='ux-labels-values__labels'):\n",
    "                key = spec.text.strip()\n",
    "                # Recherche de la valeur associée à la clé\n",
    "                value_tag = spec.find_next('div', class_='ux-labels-values__values')\n",
    "                value = value_tag.text.strip() if value_tag else 'N/A'\n",
    "                specs[key] = value\n",
    "\n",
    "            # Dictionnaire de base pour les informations communes\n",
    "            product_details = {\n",
    "                'Title': title,\n",
    "                'Price': price,\n",
    "                'Collection Date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            }\n",
    "\n",
    "            # Ajout des spécificités en fonction de la catégorie\n",
    "            if category == \"Laptops\":\n",
    "                product_details.update({\n",
    "                    'RAM': specs.get('RAM Size', 'N/A'),\n",
    "                    'CPU': specs.get('Processor', 'N/A'),\n",
    "                    'Model': specs.get('Model', 'N/A'),\n",
    "                    'Brand': specs.get('Brand', 'N/A'),\n",
    "                    'GPU': specs.get('GPU', 'N/A'),\n",
    "                    'Screen Size': specs.get('Screen Size', 'N/A'),\n",
    "                    'Storage': specs.get('SSD Capacity', 'N/A'),\n",
    "                })\n",
    "            elif category == \"Monitors\":\n",
    "                product_details.update({\n",
    "                    'Screen Size': specs.get('Screen Size', 'N/A'),\n",
    "                    'Maximum Resolution': specs.get('Resolution', 'N/A'),\n",
    "                    'Aspect Ratio': specs.get('Aspect Ratio', 'N/A'),\n",
    "                    'Refresh Rate': specs.get('Refresh Rate', 'N/A'),\n",
    "                    'Response Time': specs.get('Response Time', 'N/A'),\n",
    "                    'Brand': specs.get('Brand', 'N/A'),\n",
    "                    'Model': specs.get('Model', 'N/A'),\n",
    "                })\n",
    "            elif category == \"Smart Watches\":\n",
    "                product_details.update({\n",
    "                    'Case Size': specs.get('Case Size', 'N/A'),\n",
    "                    'Battery Capacity': specs.get('Battery Capacity', 'N/A'),\n",
    "                    'Brand': specs.get('Brand', 'N/A'),\n",
    "                    'Model': specs.get('Model', 'N/A'),\n",
    "                    'Operating System': specs.get('Operating System', 'N/A'),\n",
    "                    'Storage Capacity': specs.get('Storage Capacity', 'N/A')\n",
    "                })\n",
    "            elif category == \"Graphics Cards\":\n",
    "                product_details.update({\n",
    "                    'Brand': specs.get('Brand', 'N/A'),\n",
    "                    'Memory Size': specs.get('Memory Size', 'N/A'),\n",
    "                    'Memory Type': specs.get('Memory Type', 'N/A'),\n",
    "                    'Chipset/GPU Model': specs.get('Chipset/GPU Model', 'N/A'),\n",
    "                    'Connectors': specs.get('Connectors', 'N/A')\n",
    "                })\n",
    "\n",
    "            print(f\"Successfully scraped {category}: {title[:50]}...\")\n",
    "            return product_details\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {product_url}: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b934614c6f0f6feb",
   "metadata": {},
   "source": [
    "## Scraping d'une Page de Recherche\n",
    "\n",
    "La fonction `scrape_search_page` effectue le scraping d'une page de résultats pour un terme de recherche donné :\n",
    "- Elle construit l'URL avec les paramètres nécessaires.\n",
    "- Elle envoie une requête GET asynchrone.\n",
    "- Elle parse le HTML pour extraire les URLs des produits présents sur la page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d17c5e3b99dceca",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_search_page(session, query, page, semaphore, category):\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            base_url = \"https://www.ebay.com/sch/i.html\"\n",
    "            params = {'_nkw': query, '_sacat': 0, '_from': 'R40', '_pgn': page}\n",
    "            headers = get_headers()\n",
    "\n",
    "            async with session.get(base_url, params=params, headers=headers) as response:\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(await response.text(), 'html.parser')\n",
    "\n",
    "                # Extraction des URLs des produits\n",
    "                items = soup.find_all('div', class_='s-item__wrapper')\n",
    "                product_urls = [item.find('a', class_='s-item__link')['href']\n",
    "                                for item in items if item.find('a', class_='s-item__link')]\n",
    "\n",
    "                print(f\"Scraped page {page} for {category} ({len(product_urls)} products)\")\n",
    "                return product_urls\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping page {page} for {category}: {str(e)}\")\n",
    "            return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474d9fbcbb66a61b",
   "metadata": {},
   "source": [
    "## Orchestration du Scraping pour Plusieurs Catégories\n",
    "\n",
    "La fonction `scrape_ebay_search` permet de :\n",
    "- Lancer le scraping sur plusieurs pages pour chaque catégorie.\n",
    "- Rassembler toutes les URLs des produits puis récupérer leurs détails.\n",
    "- Retourner un dictionnaire regroupant les produits par catégorie.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a32b920d69770a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_ebay_search(categories, max_pages=1):\n",
    "    all_products = {}\n",
    "    semaphore = asyncio.Semaphore(2)  # Limiter le nombre de requêtes simultanées\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for category, query in categories.items():\n",
    "            print(f\"\\n{'=' * 30}\\nStarting {category} scraping\\n{'=' * 30}\")\n",
    "            tasks = [scrape_search_page(session, query, page, semaphore, category)\n",
    "                     for page in range(1, max_pages + 1)]\n",
    "\n",
    "            search_results = await asyncio.gather(*tasks)\n",
    "            product_urls = [url for sublist in search_results for url in sublist]\n",
    "\n",
    "            product_tasks = [scrape_product_details(session, url, category) for url in product_urls]\n",
    "            products = await asyncio.gather(*product_tasks)\n",
    "\n",
    "            all_products[category] = [p for p in products if p]\n",
    "            print(f\"\\n{'=' * 30}\\nCompleted {category} ({len(all_products[category])} items)\\n{'=' * 30}\")\n",
    "\n",
    "    return all_products\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb96fd5c7d8df9d8",
   "metadata": {},
   "source": [
    "## Sauvegarde des Données en CSV\n",
    "\n",
    "Deux fonctions sont définies ici :\n",
    "- `get_next_scrape_number` : Pour déterminer le numéro de scrape suivant dans le dossier de sauvegarde.\n",
    "- `save_to_csv` : Pour enregistrer les données extraites dans un fichier CSV.\n",
    "Les fichiers CSV sont organisés par catégorie et contiennent les informations extraites.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7558ea2b9c0acf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_scrape_number(save_directory, category):\n",
    "    \"\"\"Détermine le prochain numéro de scrape pour un dossier donné.\"\"\"\n",
    "    scrape_number = 1\n",
    "    for filename in os.listdir(save_directory):\n",
    "        if filename.startswith(f\"{category}_\") and filename.endswith(\".csv\"):\n",
    "            try:\n",
    "                # Extraction du numéro de scrape à partir du nom du fichier\n",
    "                current_number = int(filename.split('_scrape')[-1].split('.')[0])\n",
    "                if current_number >= scrape_number:\n",
    "                    scrape_number = current_number + 1\n",
    "            except ValueError:\n",
    "                continue\n",
    "    return scrape_number\n",
    "\n",
    "def save_to_csv(data, category, save_directory, fieldnames):\n",
    "    # Formatage du nom du dossier et du fichier\n",
    "    category_folder = category.lower().replace(' ', '_')\n",
    "    category_filename = category_folder\n",
    "    today_date = datetime.now().strftime('%Y_%m_%d')\n",
    "\n",
    "    # Création du dossier de sauvegarde s'il n'existe pas\n",
    "    category_directory = os.path.join(save_directory, category_folder)\n",
    "    os.makedirs(category_directory, exist_ok=True)\n",
    "\n",
    "    # Détermination du prochain numéro de scrape\n",
    "    scrape_number = get_next_scrape_number(category_directory, category_filename)\n",
    "\n",
    "    # Génération du nom du fichier CSV\n",
    "    filename = os.path.join(category_directory, f\"{category_filename}_{today_date}_scrape{scrape_number}.csv\")\n",
    "\n",
    "    # Sauvegarde des données au format CSV\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "    print(f\"Saved {len(data)} {category} items to {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81beb03dd4c8bef",
   "metadata": {},
   "source": [
    "## Fonction Principale et Exécution du Scraping\n",
    "\n",
    "La fonction `main` orchestre l'ensemble du processus :\n",
    "1. Définition des catégories et des requêtes associées.\n",
    "2. Lancement du scraping asynchrone sur plusieurs pages pour chaque catégorie.\n",
    "3. Sauvegarde des résultats dans des fichiers CSV organisés par catégorie.\n",
    "\n",
    "**Note :**\n",
    "Si vous exécutez ce code dans un notebook Jupyter et rencontrez une erreur relative à l'event loop, utilisez le module `nest_asyncio` pour permettre l'exécution de boucles asynchrones imbriquées.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d709216a40c0bf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    categories = {\n",
    "        \"Laptops\": \"laptop\",\n",
    "        \"Monitors\": \"monitor\",\n",
    "        \"Smart Watches\": \"smart watch\",\n",
    "        \"Graphics Cards\": \"graphics card\"\n",
    "    }\n",
    "\n",
    "    max_pages = 18  # Nombre de pages à scraper par catégorie\n",
    "    save_directory = \"data/raw/ebay\"\n",
    "\n",
    "    print(\"\\nStarting eBay scraping...\")\n",
    "    all_products = await scrape_ebay_search(categories, max_pages)\n",
    "\n",
    "    # Définition des colonnes attendues pour chaque catégorie\n",
    "    category_fields = {\n",
    "        \"Laptops\": ['Title', 'Price', 'RAM', 'CPU', 'Model', 'Brand', 'GPU', 'Screen Size', 'Storage', 'Collection Date'],\n",
    "        \"Monitors\": ['Title', 'Price', 'Screen Size', 'Maximum Resolution', 'Aspect Ratio', 'Refresh Rate', 'Response Time', 'Brand', 'Model', 'Collection Date'],\n",
    "        \"Smart Watches\": ['Title', 'Price', 'Case Size', 'Battery Capacity', 'Brand', 'Model', 'Operating System', 'Storage Capacity', 'Collection Date'],\n",
    "        \"Graphics Cards\": ['Title', 'Price', 'Brand', 'Memory Size', 'Memory Type', 'Chipset/GPU Model', 'Connectors', 'Collection Date']\n",
    "    }\n",
    "\n",
    "    # Sauvegarde des données pour chaque catégorie\n",
    "    for category, products in all_products.items():\n",
    "        if products:\n",
    "            save_to_csv(products, category, save_directory, category_fields[category])\n",
    "\n",
    "# Exécution de la fonction principale.\n",
    "# Dans un script Python, on utiliserait :\n",
    "# if __name__ == \"__main__\":\n",
    "#     asyncio.run(main())\n",
    "#\n",
    "# Pour Jupyter Notebook, en cas d'erreur liée à l'event loop, on peut utiliser nest_asyncio :\n",
    "try:\n",
    "    asyncio.run(main())\n",
    "except RuntimeError as e:\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "    await main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d37349a0001b923",
   "metadata": {},
   "source": [
    "##  interprétations des résultats.\n",
    "\n",
    "Ce notebook vous permet de :\n",
    "- **Scraper** des données de produits sur eBay pour différentes catégories de produits.\n",
    "- **Sauvegarder** ces données dans des fichiers CSV organisés par catégorie.\n",
    "- **Analyser** les premiers résultats (vous pouvez par la suite utiliser des bibliothèques comme `pandas` pour approfondir l'analyse).\n",
    "\n",
    "N'oubliez pas d'installer les dépendances nécessaires (par exemple via `!pip install aiohttp bs4 fake_useragent nest_asyncio`) avant d'exécuter ce notebook.\n",
    "\n",
    "Voila example de donner   scraping pour seule sous category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a49f7f5cda8394d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T18:30:01.492277Z",
     "start_time": "2025-02-08T18:29:59.538976Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Brand</th>\n",
       "      <th>Memory Size</th>\n",
       "      <th>Memory Type</th>\n",
       "      <th>Chipset/GPU Model</th>\n",
       "      <th>Connectors</th>\n",
       "      <th>Collection Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XFX SPEEDSTER SWFT 309 Radeon RX 6700 XT 12GB ...</td>\n",
       "      <td>$250.00</td>\n",
       "      <td>XFX</td>\n",
       "      <td>12 GB</td>\n",
       "      <td>GDDR6</td>\n",
       "      <td>AMD Radeon RX 6700 XT</td>\n",
       "      <td>DisplayPort, HDMI</td>\n",
       "      <td>1/29/2025 22:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MSI NVIDIA GeForce RTX 3090 24GB GDRR6X Gaming...</td>\n",
       "      <td>$950.00</td>\n",
       "      <td>MSI</td>\n",
       "      <td>24 GB</td>\n",
       "      <td>GDDR6X</td>\n",
       "      <td>NVIDIA GeForce RTX 3090</td>\n",
       "      <td>DisplayPort, HDMI</td>\n",
       "      <td>1/29/2025 22:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dell AMD Radeon HD8490 1GB DDR3 Graphics Video...</td>\n",
       "      <td>$9.00</td>\n",
       "      <td>Dell</td>\n",
       "      <td>1 GB</td>\n",
       "      <td>DDR3</td>\n",
       "      <td>AMD Radeon HD 8490</td>\n",
       "      <td>DisplayPort, DVI, DVI-I</td>\n",
       "      <td>1/29/2025 22:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SAPPHIRE PULSE Radeon RX 580 8GB GDDR5 Graphic...</td>\n",
       "      <td>GBP 70.00</td>\n",
       "      <td>SAPPHIRE</td>\n",
       "      <td>8 GB</td>\n",
       "      <td>GDDR5</td>\n",
       "      <td>AMD Radeon RX 580</td>\n",
       "      <td>DisplayPort, DVI-D, HDMI</td>\n",
       "      <td>1/29/2025 22:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>YESTON RX 6400 RX6400 4GB GDDR6 Graphics Card</td>\n",
       "      <td>$139.80/ea</td>\n",
       "      <td>YESTYON</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AMD RX 6400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/29/2025 22:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AMD Radeon RX 580 8GB GDDR5 2048SP 256Bit PCI-...</td>\n",
       "      <td>$105.77</td>\n",
       "      <td>Graphics</td>\n",
       "      <td>8 GB</td>\n",
       "      <td>GDDR5</td>\n",
       "      <td>AMD Radeon RX 580</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/29/2025 22:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AORUS Radeon RX 580 XTR 8G 8GB GDDR5 Video Car...</td>\n",
       "      <td>$99.94</td>\n",
       "      <td>GIGABYTE</td>\n",
       "      <td>8 GB</td>\n",
       "      <td>GDDR5</td>\n",
       "      <td>AMD Radeon RX 580</td>\n",
       "      <td>DisplayPort, DVI, DVI-D, HDMI</td>\n",
       "      <td>1/29/2025 22:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AMD Radeon Pro WX 3100 4GB GDDR5 Graphics Card</td>\n",
       "      <td>$20.00</td>\n",
       "      <td>HP</td>\n",
       "      <td>4 GB</td>\n",
       "      <td>GDDR5</td>\n",
       "      <td>Radeon Pro WX 3100</td>\n",
       "      <td>DisplayPort, Mini DisplayPort</td>\n",
       "      <td>1/29/2025 22:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Zotac GeForce GTX 1070 8GB Mini Graphics Card ...</td>\n",
       "      <td>$119.95/ea</td>\n",
       "      <td>Zotac</td>\n",
       "      <td>8 GB</td>\n",
       "      <td>GDDR5</td>\n",
       "      <td>Nvidia GeForce GTX 1070</td>\n",
       "      <td>DisplayPort, HDMI</td>\n",
       "      <td>1/29/2025 22:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GIGABYTE GeForce GTX 1660 SUPER Gaming OC 6GB ...</td>\n",
       "      <td>$109.99</td>\n",
       "      <td>GIGABYTE</td>\n",
       "      <td>6 GB</td>\n",
       "      <td>GDDR6</td>\n",
       "      <td>NVIDIA GeForce GTX1660 SUPER</td>\n",
       "      <td>DisplayPort, HDMI</td>\n",
       "      <td>1/29/2025 22:26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title       Price     Brand  \\\n",
       "0  XFX SPEEDSTER SWFT 309 Radeon RX 6700 XT 12GB ...    $250.00        XFX   \n",
       "1  MSI NVIDIA GeForce RTX 3090 24GB GDRR6X Gaming...    $950.00        MSI   \n",
       "2  Dell AMD Radeon HD8490 1GB DDR3 Graphics Video...      $9.00       Dell   \n",
       "3  SAPPHIRE PULSE Radeon RX 580 8GB GDDR5 Graphic...   GBP 70.00  SAPPHIRE   \n",
       "4      YESTON RX 6400 RX6400 4GB GDDR6 Graphics Card  $139.80/ea   YESTYON   \n",
       "5  AMD Radeon RX 580 8GB GDDR5 2048SP 256Bit PCI-...    $105.77   Graphics   \n",
       "6  AORUS Radeon RX 580 XTR 8G 8GB GDDR5 Video Car...     $99.94   GIGABYTE   \n",
       "7     AMD Radeon Pro WX 3100 4GB GDDR5 Graphics Card     $20.00         HP   \n",
       "8  Zotac GeForce GTX 1070 8GB Mini Graphics Card ...  $119.95/ea     Zotac   \n",
       "9  GIGABYTE GeForce GTX 1660 SUPER Gaming OC 6GB ...    $109.99   GIGABYTE   \n",
       "\n",
       "  Memory Size Memory Type             Chipset/GPU Model  \\\n",
       "0       12 GB       GDDR6         AMD Radeon RX 6700 XT   \n",
       "1       24 GB      GDDR6X       NVIDIA GeForce RTX 3090   \n",
       "2        1 GB        DDR3            AMD Radeon HD 8490   \n",
       "3        8 GB       GDDR5             AMD Radeon RX 580   \n",
       "4         NaN         NaN                   AMD RX 6400   \n",
       "5        8 GB       GDDR5             AMD Radeon RX 580   \n",
       "6        8 GB       GDDR5             AMD Radeon RX 580   \n",
       "7        4 GB       GDDR5            Radeon Pro WX 3100   \n",
       "8        8 GB       GDDR5       Nvidia GeForce GTX 1070   \n",
       "9        6 GB       GDDR6  NVIDIA GeForce GTX1660 SUPER   \n",
       "\n",
       "                      Connectors  Collection Date  \n",
       "0              DisplayPort, HDMI  1/29/2025 22:26  \n",
       "1              DisplayPort, HDMI  1/29/2025 22:26  \n",
       "2        DisplayPort, DVI, DVI-I  1/29/2025 22:26  \n",
       "3       DisplayPort, DVI-D, HDMI  1/29/2025 22:26  \n",
       "4                            NaN  1/29/2025 22:26  \n",
       "5                            NaN  1/29/2025 22:26  \n",
       "6  DisplayPort, DVI, DVI-D, HDMI  1/29/2025 22:26  \n",
       "7  DisplayPort, Mini DisplayPort  1/29/2025 22:26  \n",
       "8              DisplayPort, HDMI  1/29/2025 22:26  \n",
       "9              DisplayPort, HDMI  1/29/2025 22:26  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Définir le chemin vers le fichier CSV nettoyé\n",
    "csv_file_path = Path(r'C:\\Users\\AdMin\\Desktop\\ecommerce_scraper\\data\\raw\\ebay\\graphics_cards\\graphics_cards_2025_01_29_scrape1.csv')\n",
    "\n",
    "df_cleaned = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Set the option to display all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Display all rows of the DataFrame\n",
    "df_cleaned.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
