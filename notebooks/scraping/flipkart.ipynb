{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#  Scraping sur Flipkart\n",
    "\n",
    "Ce notebook a pour objectif de scraper des informations sur Flipkart pour différentes catégories de produits (graphics_cards, laptops, monitors, smart_watches).\n",
    "\n",
    "**Fonctionnalités :**\n",
    "- Récupération des détails d'un produit (spécifications, ratings, reviews, etc.).\n",
    "- Extraction des informations depuis une page de résultats.\n",
    "- Sauvegarde des données extraites dans des fichiers CSV organisés par catégorie.\n",
    "\n",
    "**Bibliothèques utilisées :**\n",
    "- `os`, `time`, `random` pour la gestion du système et des délais.\n",
    "- `requests` pour envoyer des requêtes HTTP.\n",
    "- `BeautifulSoup` (via `bs4`) pour parser le HTML.\n",
    "- `pandas` pour la manipulation et la sauvegarde des données.\n",
    "- `datetime` pour la gestion des dates.\n",
    "- `re` pour les expressions régulières.\n"
   ],
   "id": "69d3665640a05a43"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n"
   ],
   "id": "8cb7a322ac5e577a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Constants et Headers\n",
    "\n",
    "Nous définissons ici quelques constantes essentielles :\n",
    "- **USER_AGENTS** : Une liste de User-Agent pour simuler des requêtes provenant de différents navigateurs.\n",
    "- **DEFAULT_HEADERS** : Les en-têtes HTTP utilisés pour nos requêtes, incluant un User-Agent choisi aléatoirement.\n"
   ],
   "id": "bee2903ba9688f99"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Liste de User-Agent pour varier les requêtes\n",
    "USER_AGENTS = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Mozilla/5.0 (iPhone; CPU iPhone OS 14_4 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1',\n",
    "]\n",
    "\n",
    "DEFAULT_HEADERS = {\n",
    "    'User-Agent': random.choice(USER_AGENTS),\n",
    "    'Accept-Language': 'en-US, en;q=0.5'\n",
    "}\n"
   ],
   "id": "9161021186120c3b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Fonctions Utilitaires\n",
    "\n",
    "Ces fonctions facilitent l'extraction et le traitement des données :\n",
    "- **get_text_or_default** : Extrait le texte d'un élément BeautifulSoup ou renvoie une valeur par défaut.\n",
    "- **wait_random** : Introduit une attente aléatoire pour éviter d'être détecté.\n",
    "- **extract_specifications** : Extrait les spécifications d'un produit à partir du HTML.\n"
   ],
   "id": "de66d226e0e4c96e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_text_or_default(element, default=\"Data not available\"):\n",
    "    \"\"\"Extrait le texte d'un élément BeautifulSoup ou renvoie une valeur par défaut.\"\"\"\n",
    "    return element.text.strip() if element else default\n",
    "\n",
    "def wait_random(min_time=3, max_time=7):\n",
    "    \"\"\"Attend un nombre de secondes aléatoire pour éviter la détection.\"\"\"\n",
    "    delay = random.randint(min_time, max_time)\n",
    "    print(f\"Waiting {delay} seconds...\")\n",
    "    time.sleep(delay)\n",
    "\n",
    "def extract_specifications(soup):\n",
    "    \"\"\"Extrait les spécifications de la section de détails d'un produit.\"\"\"\n",
    "    specifications = {}\n",
    "    spec_sections = soup.find_all('div', class_='GNDEQ-')\n",
    "\n",
    "    for section in spec_sections:\n",
    "        section_title = section.find('div', class_='_4BJ2V+')\n",
    "        if not section_title:\n",
    "            continue\n",
    "        section_title = section_title.text.strip()\n",
    "\n",
    "        spec_rows = section.find_all('tr', class_='WJdYP6 row')\n",
    "        for row in spec_rows:\n",
    "            key_element = row.find('td', class_='+fFi1w col col-3-12')\n",
    "            value_element = row.find('td', class_='Izz52n col col-9-12')\n",
    "\n",
    "            if key_element and value_element:\n",
    "                key = key_element.text.strip()\n",
    "                value = value_element.text.strip()\n",
    "                specifications[key] = value\n",
    "\n",
    "    return specifications\n"
   ],
   "id": "6b93c00d3112630e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Scraping des Détails d'un Produit\n",
    "\n",
    "La fonction `scrape_flipkart_product` effectue les opérations suivantes :\n",
    "- Envoie une requête HTTP pour obtenir la page du produit.\n",
    "- Parse le HTML et extrait les spécifications via `extract_specifications`.\n",
    "- Récupère la note (rating) et le nombre d'avis (reviews) à l'aide d'expressions régulières.\n",
    "- Retourne un dictionnaire regroupant ces informations.\n"
   ],
   "id": "2182f2999180a09a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def scrape_flipkart_product(product_url):\n",
    "    \"\"\"Scrape les informations détaillées d'un produit (spécifications, rating, reviews).\"\"\"\n",
    "    headers = DEFAULT_HEADERS\n",
    "    try:\n",
    "        response = requests.get(product_url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        specifications = extract_specifications(soup)\n",
    "\n",
    "        rating_element = soup.find('div', class_='_3LWZlK')\n",
    "        rating = get_text_or_default(rating_element)\n",
    "\n",
    "        reviews_element = soup.find('span', class_='_2_R_DZ')\n",
    "        reviews_text = get_text_or_default(reviews_element)\n",
    "\n",
    "        reviews_match = re.search(r'\\d+', reviews_text.replace(',', ''))\n",
    "        reviews = reviews_match.group() if reviews_match else \"Data not available\"\n",
    "\n",
    "        return {\n",
    "            \"rating\": rating,\n",
    "            \"reviews\": reviews,\n",
    "            **specifications,\n",
    "        }\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error occurred while scraping product {product_url}: {e}\")\n",
    "        return {\"rating\": \"Data not available\", \"reviews\": \"Data not available\"}\n"
   ],
   "id": "2092644b75c2c74d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Scraping d'une Page de Résultats\n",
    "\n",
    "La fonction `scrape_flipkart_page` :\n",
    "- Envoie une requête pour obtenir une page de résultats d'une catégorie donnée.\n",
    "- Identifie les blocs produits et extrait pour chacun les informations telles que le titre, le prix, le rating, les reviews, l'image et l'URL du produit.\n",
    "- Pour chaque produit, elle peut également appeler `scrape_flipkart_product` afin d'obtenir des spécifications détaillées.\n"
   ],
   "id": "dd07663acd535789"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def scrape_flipkart_page(url, category_name):\n",
    "    headers = DEFAULT_HEADERS\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        product_blocks = soup.find_all('div', class_='cPHDOP col-12-12')\n",
    "        if not product_blocks:\n",
    "            print(\"No product blocks found on this page.\")\n",
    "            return []\n",
    "\n",
    "        scraped_items = []\n",
    "        collection_date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        for product in product_blocks:\n",
    "            if category_name == \"graphics_cards\":\n",
    "                title_element = product.find('a', class_='wjcEIp')\n",
    "                price_element = product.find('div', class_='Nx9bqj')\n",
    "                rating_element = product.find('div', class_='XQDdHH')\n",
    "                reviews_element = product.find('span', class_='Wphh3N')\n",
    "                image_element = product.find('img', class_='DByuf4')\n",
    "                link_element = product.find('a', class_='VJA3rP')\n",
    "            elif category_name == \"laptops\":\n",
    "                title_element = product.find('div', class_='KzDlHZ')\n",
    "                price_element = product.find('div', class_='Nx9bqj _4b5DiR')\n",
    "                rating_element = product.find('div', class_='XQDdHH')\n",
    "                reviews_element = product.find('span', class_='Wphh3N')\n",
    "                image_element = product.find('img', class_='DByuf4')\n",
    "                link_element = product.find('a', class_='CGtC98')\n",
    "            elif category_name == \"monitors\":\n",
    "                title_element = product.find('div', class_='KzDlHZ')\n",
    "                price_element = product.find('div', class_='Nx9bqj _4b5DiR')\n",
    "                rating_element = product.find('div', class_='XQDdHH')\n",
    "                reviews_element = product.find('span', class_='Wphh3N')\n",
    "                image_element = product.find('img', class_='DByuf4')\n",
    "                link_element = product.find('a', class_='CGtC98')\n",
    "            elif category_name == \"smart_watches\":\n",
    "                title_element = product.find('a', class_='WKTcLC')\n",
    "                price_element = product.find('div', class_='Nx9bqj')\n",
    "                rating_element = product.find('div', class_='XQDdHH')\n",
    "                reviews_element = product.find('span', class_='Wphh3N')\n",
    "                image_element = product.find('img', class_='_53J4C-')\n",
    "                link_element = product.find('a', class_='rPDeLR')\n",
    "\n",
    "            title = get_text_or_default(title_element)\n",
    "            price = get_text_or_default(price_element)\n",
    "            rating = get_text_or_default(rating_element)\n",
    "            reviews = get_text_or_default(reviews_element)\n",
    "            image_url = image_element['src'] if image_element else \"Image not available\"\n",
    "            product_url = f\"https://www.flipkart.com{link_element['href']}\" if link_element else \"URL not available\"\n",
    "\n",
    "            specifications = scrape_flipkart_product(product_url) if product_url != \"URL not available\" else {}\n",
    "\n",
    "            scraped_items.append({\n",
    "                \"title\": title,\n",
    "                \"price\": price,\n",
    "                \"rating\": rating,\n",
    "                \"reviews\": reviews,\n",
    "                \"image_url\": image_url,\n",
    "                \"product_url\": product_url,\n",
    "                \"collection_date\": collection_date,\n",
    "                **specifications,\n",
    "            })\n",
    "\n",
    "        return scraped_items\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error occurred while scraping {url}: {e}\")\n",
    "        return []\n"
   ],
   "id": "b46ebd0761f3d883"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Gestion de la Numérotation des Scrapes\n",
    "\n",
    "La fonction `get_next_scrape_number` permet de déterminer le prochain numéro de scrape afin d'éviter d'écraser des fichiers existants lors de la sauvegarde.\n"
   ],
   "id": "a527509094c1dd24"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_next_scrape_number(output_dir, category_name):\n",
    "    \"\"\"Détermine le prochain numéro de scrape pour un dossier donné.\"\"\"\n",
    "    scrape_number = 1\n",
    "    for filename in os.listdir(output_dir):\n",
    "        if filename.startswith(f\"{category_name}_\") and filename.endswith(\".csv\"):\n",
    "            try:\n",
    "                # Extraction du numéro de scrape à partir du nom du fichier\n",
    "                current_number = int(filename.split('_scrape')[-1].split('.')[0])\n",
    "                if current_number >= scrape_number:\n",
    "                    scrape_number = current_number + 1\n",
    "            except ValueError:\n",
    "                continue\n",
    "    return scrape_number\n"
   ],
   "id": "4e54e6f2b530d82e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Fonction Principale de Scraping pour Flipkart\n",
    "\n",
    "La fonction `scrape_flipkart` orchestre le scraping pour une catégorie donnée :\n",
    "- Elle parcourt le nombre de pages défini.\n",
    "- Pour chaque page, elle appelle `scrape_flipkart_page` afin d'extraire les produits.\n",
    "- Elle accumule les résultats, puis sauvegarde les données dans un fichier CSV dans un dossier dédié à la catégorie.\n"
   ],
   "id": "eb57b7b8bcbdb58c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def scrape_flipkart(category_url, num_pages, category_name, output_dir=\"data/raw/flipkart\"):\n",
    "    aggregated_results = []\n",
    "    for page in range(1, num_pages + 1):\n",
    "        print(f\"Scraping page {page}...\")\n",
    "        page_url = f\"{category_url}&page={page}\"\n",
    "        page_results = scrape_flipkart_page(page_url, category_name)\n",
    "\n",
    "        if not page_results:\n",
    "            print(\"No more products found. Stopping.\")\n",
    "            break\n",
    "\n",
    "        aggregated_results.extend(page_results)\n",
    "        wait_random()\n",
    "\n",
    "    if aggregated_results:\n",
    "        today = datetime.today()\n",
    "        formatted_date = today.strftime(\"%Y_%m_%d\")\n",
    "\n",
    "        # Création du dossier spécifique à la catégorie\n",
    "        category_directory = os.path.join(output_dir, category_name)\n",
    "        os.makedirs(category_directory, exist_ok=True)\n",
    "\n",
    "        # Détermination du prochain numéro de scrape\n",
    "        scrape_number = get_next_scrape_number(category_directory, category_name)\n",
    "\n",
    "        filename = f\"{category_name}_{formatted_date}_scrape{scrape_number}.csv\"\n",
    "        output_path = os.path.join(category_directory, filename)\n",
    "\n",
    "        df = pd.DataFrame(aggregated_results)\n",
    "        df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"Data saved to {output_path}\")\n",
    "    else:\n",
    "        print(\"No data scraped.\")\n",
    "\n",
    "    return aggregated_results\n"
   ],
   "id": "e776f603d0c9bb72"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Script Principal\n",
    "\n",
    "Nous définissons ici un dictionnaire `categories` qui associe à chaque catégorie son URL de base et le nombre de pages à scraper.\n",
    "Ensuite, nous parcourons chaque catégorie et appelons la fonction `scrape_flipkart` pour récupérer et sauvegarder les données.\n"
   ],
   "id": "21c95b0de60edae1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if __name__ == \"__main__\":\n",
    "    categories = {\n",
    "        \"graphics_cards\": {\n",
    "            \"url\": \"https://www.flipkart.com/gaming-components/graphic-cards/pr?sid=4rr,tin,6zn&q=graphics+card&otracker=categorytree\",\n",
    "            \"num_pages\": 0  # Modifier le nombre de pages souhaité\n",
    "        },\n",
    "        \"laptops\": {\n",
    "            \"url\": \"https://www.flipkart.com/laptops/pr?sid=6bo,b5g&q=laptop&otracker=categorytree\",\n",
    "            \"num_pages\": 0  # Modifier le nombre de pages souhaité\n",
    "        },\n",
    "        \"monitors\": {\n",
    "            \"url\": \"https://www.flipkart.com/search?q=monitor&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\",\n",
    "            \"num_pages\": 13\n",
    "        },\n",
    "        \"smart_watches\": {\n",
    "            \"url\": \"https://www.flipkart.com/wearable-smart-devices/smart-watches/pr?sid=ajy,buh&q=smart+watches&otracker=categorytree\",\n",
    "            \"num_pages\": 13\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for category_name, config in categories.items():\n",
    "        print(f\"Scraping {category_name}...\")\n",
    "        scrape_flipkart(config[\"url\"], config[\"num_pages\"], category_name)\n"
   ],
   "id": "3fc795e3d5450e81"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "Ce notebook vous permet de :\n",
    "- **Scraper** les données de Flipkart pour différentes catégories de produits.\n",
    "- **Extraire** des informations détaillées telles que les spécifications, ratings et reviews.\n",
    "- **Sauvegarder** les résultats dans des fichiers CSV organisés par catégorie.\n",
    "\n",
    "Avant d'exécuter ce notebook, assurez-vous d'avoir installé les bibliothèques requises, par exemple en utilisant :\n"
   ],
   "id": "432bfd12ad0ca50c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
