{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed60fa590c98827",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a3cf526549f4a62",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1622139f1696c46",
   "metadata": {},
   "source": [
    "# Projet de Web Scraping sur eBay\n",
    "\n",
    "Ce notebook a pour objectif de réaliser le scraping de données sur eBay pour différentes catégories de produits (Laptops, Monitors, Smart Watches, Graphics Cards).\n",
    "Pour ce faire, nous utilisons des requêtes asynchrones avec `aiohttp` et `asyncio` ainsi que `BeautifulSoup` pour parser le contenu HTML.\n",
    "Les données extraites seront ensuite sauvegardées dans des fichiers CSV pour une exploitation ultérieure.\n",
    "\n",
    "**Bibliothèques utilisées :**\n",
    "- `aiohttp`, `asyncio` : Pour effectuer des requêtes HTTP asynchrones.\n",
    "- `BeautifulSoup` (via `bs4`) : Pour parser le HTML.\n",
    "- `csv` : Pour enregistrer les résultats dans des fichiers CSV.\n",
    "- `random` : Pour introduire des délais aléatoires et simuler un comportement humain.\n",
    "- `fake_useragent` : Pour générer des User-Agent aléatoires.\n",
    "- `datetime` : Pour gérer les dates d'extraction.\n",
    "- `os` : Pour la gestion des fichiers et dossiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54629b48768de15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import random\n",
    "from fake_useragent import UserAgent\n",
    "from datetime import datetime\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168d265da558be71",
   "metadata": {},
   "source": [
    "## Définition des Headers\n",
    "\n",
    "La fonction `get_headers()` permet de générer des en-têtes HTTP avec un User-Agent aléatoire.\n",
    "Cela aide à éviter d'être bloqué par eBay en simulant un comportement de navigateur standard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48366749640c3c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation de fake_useragent pour obtenir des User-Agent aléatoires\n",
    "ua = UserAgent()\n",
    "\n",
    "def get_headers():\n",
    "    return {\n",
    "        'User-Agent': ua.random,\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Referer': 'https://www.ebay.com/',\n",
    "        'DNT': '1'\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8ac9057f597d5f",
   "metadata": {},
   "source": [
    "## Scraping des Détails d'un Produit\n",
    "\n",
    "La fonction asynchrone `scrape_product_details` reçoit une session, l'URL du produit et la catégorie du produit.\n",
    "Elle effectue les actions suivantes :\n",
    "- Attente d'un délai aléatoire pour simuler le comportement humain.\n",
    "- Envoi d'une requête HTTP GET avec des headers.\n",
    "- Parsing du contenu HTML avec BeautifulSoup.\n",
    "- Extraction du titre, du prix et des spécifications spécifiques selon la catégorie.\n",
    "- Retourne un dictionnaire contenant les détails du produit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a49a9865c1b0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_product_details(session, product_url, category):\n",
    "    try:\n",
    "        # Attendre un délai aléatoire entre 2 et 5 secondes\n",
    "        await asyncio.sleep(random.uniform(2, 5))\n",
    "        headers = get_headers()\n",
    "\n",
    "        async with session.get(product_url, headers=headers) as response:\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(await response.text(), 'html.parser')\n",
    "\n",
    "            # Extraction du titre du produit\n",
    "            title = soup.find('h1', class_='x-item-title__mainTitle')\n",
    "            title = title.text.strip() if title else 'N/A'\n",
    "\n",
    "            # Extraction du prix\n",
    "            price = soup.find('div', class_='x-price-primary')\n",
    "            price = price.text.strip() if price else 'N/A'\n",
    "\n",
    "            # Extraction des spécifications\n",
    "            specs = {}\n",
    "            for spec in soup.find_all('div', class_='ux-labels-values__labels'):\n",
    "                key = spec.text.strip()\n",
    "                # Recherche de la valeur associée à la clé\n",
    "                value_tag = spec.find_next('div', class_='ux-labels-values__values')\n",
    "                value = value_tag.text.strip() if value_tag else 'N/A'\n",
    "                specs[key] = value\n",
    "\n",
    "            # Dictionnaire de base pour les informations communes\n",
    "            product_details = {\n",
    "                'Title': title,\n",
    "                'Price': price,\n",
    "                'Collection Date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            }\n",
    "\n",
    "            # Ajout des spécificités en fonction de la catégorie\n",
    "            if category == \"Laptops\":\n",
    "                product_details.update({\n",
    "                    'RAM': specs.get('RAM Size', 'N/A'),\n",
    "                    'CPU': specs.get('Processor', 'N/A'),\n",
    "                    'Model': specs.get('Model', 'N/A'),\n",
    "                    'Brand': specs.get('Brand', 'N/A'),\n",
    "                    'GPU': specs.get('GPU', 'N/A'),\n",
    "                    'Screen Size': specs.get('Screen Size', 'N/A'),\n",
    "                    'Storage': specs.get('SSD Capacity', 'N/A'),\n",
    "                })\n",
    "            elif category == \"Monitors\":\n",
    "                product_details.update({\n",
    "                    'Screen Size': specs.get('Screen Size', 'N/A'),\n",
    "                    'Maximum Resolution': specs.get('Resolution', 'N/A'),\n",
    "                    'Aspect Ratio': specs.get('Aspect Ratio', 'N/A'),\n",
    "                    'Refresh Rate': specs.get('Refresh Rate', 'N/A'),\n",
    "                    'Response Time': specs.get('Response Time', 'N/A'),\n",
    "                    'Brand': specs.get('Brand', 'N/A'),\n",
    "                    'Model': specs.get('Model', 'N/A'),\n",
    "                })\n",
    "            elif category == \"Smart Watches\":\n",
    "                product_details.update({\n",
    "                    'Case Size': specs.get('Case Size', 'N/A'),\n",
    "                    'Battery Capacity': specs.get('Battery Capacity', 'N/A'),\n",
    "                    'Brand': specs.get('Brand', 'N/A'),\n",
    "                    'Model': specs.get('Model', 'N/A'),\n",
    "                    'Operating System': specs.get('Operating System', 'N/A'),\n",
    "                    'Storage Capacity': specs.get('Storage Capacity', 'N/A')\n",
    "                })\n",
    "            elif category == \"Graphics Cards\":\n",
    "                product_details.update({\n",
    "                    'Brand': specs.get('Brand', 'N/A'),\n",
    "                    'Memory Size': specs.get('Memory Size', 'N/A'),\n",
    "                    'Memory Type': specs.get('Memory Type', 'N/A'),\n",
    "                    'Chipset/GPU Model': specs.get('Chipset/GPU Model', 'N/A'),\n",
    "                    'Connectors': specs.get('Connectors', 'N/A')\n",
    "                })\n",
    "\n",
    "            print(f\"Successfully scraped {category}: {title[:50]}...\")\n",
    "            return product_details\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {product_url}: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b934614c6f0f6feb",
   "metadata": {},
   "source": [
    "## Scraping d'une Page de Recherche\n",
    "\n",
    "La fonction `scrape_search_page` effectue le scraping d'une page de résultats pour un terme de recherche donné :\n",
    "- Elle construit l'URL avec les paramètres nécessaires.\n",
    "- Elle envoie une requête GET asynchrone.\n",
    "- Elle parse le HTML pour extraire les URLs des produits présents sur la page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d17c5e3b99dceca",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_search_page(session, query, page, semaphore, category):\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            base_url = \"https://www.ebay.com/sch/i.html\"\n",
    "            params = {'_nkw': query, '_sacat': 0, '_from': 'R40', '_pgn': page}\n",
    "            headers = get_headers()\n",
    "\n",
    "            async with session.get(base_url, params=params, headers=headers) as response:\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(await response.text(), 'html.parser')\n",
    "\n",
    "                # Extraction des URLs des produits\n",
    "                items = soup.find_all('div', class_='s-item__wrapper')\n",
    "                product_urls = [item.find('a', class_='s-item__link')['href']\n",
    "                                for item in items if item.find('a', class_='s-item__link')]\n",
    "\n",
    "                print(f\"Scraped page {page} for {category} ({len(product_urls)} products)\")\n",
    "                return product_urls\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping page {page} for {category}: {str(e)}\")\n",
    "            return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474d9fbcbb66a61b",
   "metadata": {},
   "source": [
    "## Orchestration du Scraping pour Plusieurs Catégories\n",
    "\n",
    "La fonction `scrape_ebay_search` permet de :\n",
    "- Lancer le scraping sur plusieurs pages pour chaque catégorie.\n",
    "- Rassembler toutes les URLs des produits puis récupérer leurs détails.\n",
    "- Retourner un dictionnaire regroupant les produits par catégorie.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a32b920d69770a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_ebay_search(categories, max_pages=1):\n",
    "    all_products = {}\n",
    "    semaphore = asyncio.Semaphore(2)  # Limiter le nombre de requêtes simultanées\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for category, query in categories.items():\n",
    "            print(f\"\\n{'=' * 30}\\nStarting {category} scraping\\n{'=' * 30}\")\n",
    "            tasks = [scrape_search_page(session, query, page, semaphore, category)\n",
    "                     for page in range(1, max_pages + 1)]\n",
    "\n",
    "            search_results = await asyncio.gather(*tasks)\n",
    "            product_urls = [url for sublist in search_results for url in sublist]\n",
    "\n",
    "            product_tasks = [scrape_product_details(session, url, category) for url in product_urls]\n",
    "            products = await asyncio.gather(*product_tasks)\n",
    "\n",
    "            all_products[category] = [p for p in products if p]\n",
    "            print(f\"\\n{'=' * 30}\\nCompleted {category} ({len(all_products[category])} items)\\n{'=' * 30}\")\n",
    "\n",
    "    return all_products\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb96fd5c7d8df9d8",
   "metadata": {},
   "source": [
    "## Sauvegarde des Données en CSV\n",
    "\n",
    "Deux fonctions sont définies ici :\n",
    "- `get_next_scrape_number` : Pour déterminer le numéro de scrape suivant dans le dossier de sauvegarde.\n",
    "- `save_to_csv` : Pour enregistrer les données extraites dans un fichier CSV.\n",
    "Les fichiers CSV sont organisés par catégorie et contiennent les informations extraites.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7558ea2b9c0acf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_scrape_number(save_directory, category):\n",
    "    \"\"\"Détermine le prochain numéro de scrape pour un dossier donné.\"\"\"\n",
    "    scrape_number = 1\n",
    "    for filename in os.listdir(save_directory):\n",
    "        if filename.startswith(f\"{category}_\") and filename.endswith(\".csv\"):\n",
    "            try:\n",
    "                # Extraction du numéro de scrape à partir du nom du fichier\n",
    "                current_number = int(filename.split('_scrape')[-1].split('.')[0])\n",
    "                if current_number >= scrape_number:\n",
    "                    scrape_number = current_number + 1\n",
    "            except ValueError:\n",
    "                continue\n",
    "    return scrape_number\n",
    "\n",
    "def save_to_csv(data, category, save_directory, fieldnames):\n",
    "    # Formatage du nom du dossier et du fichier\n",
    "    category_folder = category.lower().replace(' ', '_')\n",
    "    category_filename = category_folder\n",
    "    today_date = datetime.now().strftime('%Y_%m_%d')\n",
    "\n",
    "    # Création du dossier de sauvegarde s'il n'existe pas\n",
    "    category_directory = os.path.join(save_directory, category_folder)\n",
    "    os.makedirs(category_directory, exist_ok=True)\n",
    "\n",
    "    # Détermination du prochain numéro de scrape\n",
    "    scrape_number = get_next_scrape_number(category_directory, category_filename)\n",
    "\n",
    "    # Génération du nom du fichier CSV\n",
    "    filename = os.path.join(category_directory, f\"{category_filename}_{today_date}_scrape{scrape_number}.csv\")\n",
    "\n",
    "    # Sauvegarde des données au format CSV\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "    print(f\"Saved {len(data)} {category} items to {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81beb03dd4c8bef",
   "metadata": {},
   "source": [
    "## Fonction Principale et Exécution du Scraping\n",
    "\n",
    "La fonction `main` orchestre l'ensemble du processus :\n",
    "1. Définition des catégories et des requêtes associées.\n",
    "2. Lancement du scraping asynchrone sur plusieurs pages pour chaque catégorie.\n",
    "3. Sauvegarde des résultats dans des fichiers CSV organisés par catégorie.\n",
    "\n",
    "**Note :**\n",
    "Si vous exécutez ce code dans un notebook Jupyter et rencontrez une erreur relative à l'event loop, utilisez le module `nest_asyncio` pour permettre l'exécution de boucles asynchrones imbriquées.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d709216a40c0bf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    categories = {\n",
    "        \"Laptops\": \"laptop\",\n",
    "        \"Monitors\": \"monitor\",\n",
    "        \"Smart Watches\": \"smart watch\",\n",
    "        \"Graphics Cards\": \"graphics card\"\n",
    "    }\n",
    "\n",
    "    max_pages = 18  # Nombre de pages à scraper par catégorie\n",
    "    save_directory = \"data/raw/ebay\"\n",
    "\n",
    "    print(\"\\nStarting eBay scraping...\")\n",
    "    all_products = await scrape_ebay_search(categories, max_pages)\n",
    "\n",
    "    # Définition des colonnes attendues pour chaque catégorie\n",
    "    category_fields = {\n",
    "        \"Laptops\": ['Title', 'Price', 'RAM', 'CPU', 'Model', 'Brand', 'GPU', 'Screen Size', 'Storage', 'Collection Date'],\n",
    "        \"Monitors\": ['Title', 'Price', 'Screen Size', 'Maximum Resolution', 'Aspect Ratio', 'Refresh Rate', 'Response Time', 'Brand', 'Model', 'Collection Date'],\n",
    "        \"Smart Watches\": ['Title', 'Price', 'Case Size', 'Battery Capacity', 'Brand', 'Model', 'Operating System', 'Storage Capacity', 'Collection Date'],\n",
    "        \"Graphics Cards\": ['Title', 'Price', 'Brand', 'Memory Size', 'Memory Type', 'Chipset/GPU Model', 'Connectors', 'Collection Date']\n",
    "    }\n",
    "\n",
    "    # Sauvegarde des données pour chaque catégorie\n",
    "    for category, products in all_products.items():\n",
    "        if products:\n",
    "            save_to_csv(products, category, save_directory, category_fields[category])\n",
    "\n",
    "# Exécution de la fonction principale.\n",
    "# Dans un script Python, on utiliserait :\n",
    "# if __name__ == \"__main__\":\n",
    "#     asyncio.run(main())\n",
    "#\n",
    "# Pour Jupyter Notebook, en cas d'erreur liée à l'event loop, on peut utiliser nest_asyncio :\n",
    "try:\n",
    "    asyncio.run(main())\n",
    "except RuntimeError as e:\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "    await main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d37349a0001b923",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Ce notebook vous permet de :\n",
    "- **Scraper** des données de produits sur eBay pour différentes catégories de produits.\n",
    "- **Sauvegarder** ces données dans des fichiers CSV organisés par catégorie.\n",
    "- **Analyser** les premiers résultats (vous pouvez par la suite utiliser des bibliothèques comme `pandas` pour approfondir l'analyse).\n",
    "\n",
    "N'oubliez pas d'installer les dépendances nécessaires (par exemple via `!pip install aiohttp bs4 fake_useragent nest_asyncio`) avant d'exécuter ce notebook.\n",
    "\n",
    "Bonne utilisation et bon scraping !\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
