{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#  Scraping sur Ubuy\n",
    "\n",
    "Ce notebook permet de scraper des informations détaillées sur des produits depuis le site **Ubuy** en utilisant Selenium avec _undetected_chromedriver_.\n",
    "Le script réalise les opérations suivantes :\n",
    "\n",
    "- **Initialisation du navigateur** en mode \"undetected\" pour contourner les mesures anti-bot.\n",
    "- **Gestion de CAPTCHA** : Si un CAPTCHA est détecté, l'utilisateur est invité à le résoudre manuellement.\n",
    "- **Scraping des détails produits** : Extraction des spécifications techniques à partir des pages produit.\n",
    "- **Navigation multi-pages** : Parcours des pages d'un catalogue de produits.\n",
    "- **Sauvegarde des données** dans un fichier CSV, avec gestion de la version (numérotation des scrapes).\n",
    "\n",
    "Les bibliothèques principales utilisées sont :\n",
    "- `selenium` et `undetected_chromedriver` pour automatiser le navigateur.\n",
    "- `BeautifulSoup` pour le parsing du HTML.\n",
    "- `csv`, `os` et `datetime` pour la sauvegarde des données.\n",
    "- `ThreadPoolExecutor` pour exécuter des tâches en parallèle (lors du scraping des détails produits).\n",
    "- `logging` pour le suivi des opérations.\n",
    "\n",
    "Chaque fonction du script est commentée en détail afin de faciliter sa compréhension.\n"
   ],
   "id": "f8279756c987b02f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import undetected_chromedriver as uc\n"
   ],
   "id": "f53dcc7f60763089"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Configuration du Logging\n",
    "\n",
    "Nous utilisons le module `logging` pour afficher des messages informatifs sur l'état d'exécution du script.\n",
    "Cela permet de suivre l'initialisation du driver, le scraping de chaque page ou produit, ainsi que les éventuelles erreurs.\n"
   ],
   "id": "b3b106f1753dcaa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
   "id": "6b9e91b4dc33021b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Fonction `get_random_user_agent`\n",
    "\n",
    "Cette fonction retourne un User-Agent choisi aléatoirement à partir d'une liste prédéfinie.\n",
    "Cela permet de modifier régulièrement le User-Agent envoyé dans les requêtes afin d'éviter la détection par le site.\n"
   ],
   "id": "d9012a70be1b97e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_random_user_agent():\n",
    "    \"\"\"Retourne un user-agent choisi aléatoirement pour éviter la détection.\"\"\"\n",
    "    user_agents = [\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\"\n",
    "    ]\n",
    "    return random.choice(user_agents)\n"
   ],
   "id": "5aed3d20fe58756e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Fonction `get_driver`\n",
    "\n",
    "Cette fonction initialise et retourne une instance d'Undetected ChromeDriver.\n",
    "Les options du navigateur sont configurées pour :\n",
    "- Lancer le navigateur en mode non-headless (pour le debugging, vous pouvez passer en headless en modifiant l'option).\n",
    "- Définir la taille de la fenêtre, désactiver certaines optimisations susceptibles de trahir l'automatisation et ajouter un User-Agent aléatoire.\n",
    "- Utiliser le _ChromeDriverManager_ pour gérer automatiquement l'installation du driver.\n",
    "\n",
    "En cas d'erreur lors de l'initialisation, un message d'erreur est loggé et l'exception est propagée.\n"
   ],
   "id": "2de5c51a9548d832"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_driver():\n",
    "    \"\"\"Initialise une instance d'Undetected ChromeDriver.\"\"\"\n",
    "    try:\n",
    "        logging.info(\"Initializing ChromeDriver...\")\n",
    "        options = uc.ChromeOptions()\n",
    "        options.headless = False  # Mettre sur True pour exécuter en arrière-plan\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "        options.add_argument(\"--window-size=1920x1080\")\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "        options.add_argument(f\"--user-agent={get_random_user_agent()}\")\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = uc.Chrome(service=service, options=options)\n",
    "        logging.info(\"ChromeDriver initialized successfully!\")\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to initialize ChromeDriver: {e}\")\n",
    "        raise\n"
   ],
   "id": "337c875eb34a23e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Fonction `handle_captcha`\n",
    "\n",
    "Cette fonction interrompt l'exécution du script lorsqu'un CAPTCHA est détecté,\n",
    "permettant ainsi à l'utilisateur de résoudre manuellement le CAPTCHA avant de reprendre l'exécution.\n"
   ],
   "id": "5cf3a7e58edb5fd7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def handle_captcha(driver):\n",
    "    \"\"\"Met en pause l'exécution pour permettre à l'utilisateur de résoudre le CAPTCHA.\"\"\"\n",
    "    logging.info(\"CAPTCHA detected. Please solve it manually.\")\n",
    "    input(\"Press Enter to continue after solving the CAPTCHA...\")\n",
    "    logging.info(\"Resuming script execution...\")\n"
   ],
   "id": "63f29f6cd679f6eb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Fonction `scrape_product_details`\n",
    "\n",
    "Cette fonction effectue le scraping d'une page produit en suivant ces étapes :\n",
    "\n",
    "1. **Chargement de la page** : Le driver navigue vers l'URL du produit.\n",
    "2. **Délais aléatoires** : Un délai aléatoire est appliqué pour simuler un comportement humain.\n",
    "3. **Détection du CAPTCHA** : Si un CAPTCHA est présent, l'utilisateur est invité à le résoudre.\n",
    "4. **Attente de chargement** : Le script attend que la table de spécifications (dans les sections `#additional-info` ou `#technical-info`) soit présente.\n",
    "5. **Parsing du contenu** : BeautifulSoup est utilisé pour extraire les spécifications présentes dans la table.\n",
    "6. **Retour** : Un dictionnaire contenant les spécifications est retourné.\n",
    "\n",
    "En cas d'erreur, un dictionnaire vide est retourné et l'erreur est loggée.\n"
   ],
   "id": "2fd79330b3c8e6ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def scrape_product_details(driver, product_url):\n",
    "    \"\"\"Scrape les spécifications détaillées d'un produit à partir de sa page.\"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Scraping product: {product_url}\")\n",
    "        driver.get(product_url)\n",
    "        time.sleep(random.uniform(2, 5))  # Délai aléatoire\n",
    "\n",
    "        # Vérifie la présence d'un CAPTCHA\n",
    "        try:\n",
    "            WebDriverWait(driver, 5).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"iframe[src*='captcha']\"))\n",
    "            )\n",
    "            handle_captcha(driver)  # Pause pour résolution manuelle du CAPTCHA\n",
    "        except:\n",
    "            logging.info(\"No CAPTCHA detected. Proceeding with scraping...\")\n",
    "\n",
    "        # Attente que la table de spécifications soit chargée\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"div#additional-info table, div#technical-info table\"))\n",
    "        )\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        specs = {}\n",
    "\n",
    "        # Extraction des spécifications depuis les tables\n",
    "        spec_tables = soup.select(\"div#additional-info table, div#technical-info table\")\n",
    "        for table in spec_tables:\n",
    "            for row in table.find_all(\"tr\"):\n",
    "                cols = row.find_all(\"td\")\n",
    "                if len(cols) == 2:\n",
    "                    key = cols[0].text.strip()\n",
    "                    value = cols[1].text.strip()\n",
    "                    specs[key] = value\n",
    "\n",
    "        return specs\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error scraping {product_url}: {e}\")\n",
    "        return {}\n"
   ],
   "id": "bf295b917ec61e09"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Fonction `get_next_scrape_number`\n",
    "\n",
    "Cette fonction détermine le numéro du prochain scrape en vérifiant les fichiers existants dans le dossier de sortie.\n",
    "Cela permet de versionner les fichiers CSV et d'éviter de les écraser lors de sauvegardes successives.\n"
   ],
   "id": "9e924cdd05d8e17d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_next_scrape_number(output_dir, category):\n",
    "    \"\"\"Détermine le prochain numéro de scrape pour la version d'un fichier CSV.\"\"\"\n",
    "    scrape_number = 1\n",
    "    for filename in os.listdir(output_dir):\n",
    "        if filename.startswith(f\"{category}_\") and filename.endswith(\".csv\"):\n",
    "            try:\n",
    "                current_number = int(filename.split('_scrape')[-1].split('.')[0])\n",
    "                if current_number >= scrape_number:\n",
    "                    scrape_number = current_number + 1\n",
    "            except ValueError:\n",
    "                continue\n",
    "    return scrape_number\n"
   ],
   "id": "c18c7ecde370d743"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Fonction `save_to_csv`\n",
    "\n",
    "Cette fonction enregistre les données scrapées dans un fichier CSV.\n",
    "Les principales étapes sont :\n",
    "- Création d'un répertoire spécifique à la catégorie si nécessaire.\n",
    "- Détermination du numéro de scrape (pour versionner les fichiers).\n",
    "- Construction des en-têtes du CSV (incluant les colonnes de base et toutes les clés de spécifications rencontrées).\n",
    "- Écriture de chaque ligne dans le CSV.\n"
   ],
   "id": "67ea72c9616d9df6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def save_to_csv(data, category, all_spec_keys):\n",
    "    \"\"\"Enregistre les données scrapées dans un fichier CSV.\"\"\"\n",
    "    # Création d'un répertoire dédié à la catégorie\n",
    "    output_dir = os.path.join(\"data/raw/ubuy\", category)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    today_date = datetime.today().strftime(\"%Y_%m_%d\")\n",
    "    scrape_number = get_next_scrape_number(output_dir, category)\n",
    "    filename = f\"{category}_{today_date}_scrape{scrape_number}.csv\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "    # Définition des colonnes : données de base + toutes les clés de spécifications\n",
    "    fieldnames = [\"title\", \"price\", \"image_url\", \"product_url\", \"Collection Date\"] + list(all_spec_keys)\n",
    "\n",
    "    with open(filepath, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for item in data:\n",
    "            row = {\n",
    "                \"title\": item[\"title\"],\n",
    "                \"price\": item[\"price\"],\n",
    "                \"image_url\": item[\"image_url\"],\n",
    "                \"product_url\": item[\"product_url\"],\n",
    "                \"Collection Date\": today_date\n",
    "            }\n",
    "            row.update(item[\"specifications\"])\n",
    "            writer.writerow(row)\n",
    "\n",
    "    logging.info(f\"Data saved to {filepath}\")\n"
   ],
   "id": "66b6cb172abeea2e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Fonction `scrape_ubuy`\n",
    "\n",
    "Cette fonction orchestre le scraping d'une catégorie de produits sur Ubuy.\n",
    "Les étapes réalisées sont :\n",
    "\n",
    "1. **Navigation et gestion des pages** :\n",
    "   - Le driver charge la page de base.\n",
    "   - Le script récupère les blocs produits et en extrait les URLs.\n",
    "   - Si un CAPTCHA est détecté, l'utilisateur est invité à le résoudre.\n",
    "\n",
    "2. **Scraping concurrent des détails produits** :\n",
    "   - Pour chaque URL de produit, la fonction `scrape_product_details` est appelée en parallèle (via `ThreadPoolExecutor`).\n",
    "   - Les clés de spécifications rencontrées sont collectées dans un ensemble `all_spec_keys`.\n",
    "\n",
    "3. **Navigation vers la page suivante** :\n",
    "   - Le script recherche le bouton ou l'élément permettant d'accéder à la page suivante.\n",
    "   - Si une page suivante est trouvée, l'URL est mise à jour, sinon le scraping s'arrête.\n",
    "\n",
    "4. **Fermeture du driver** :\n",
    "   - Une fois le scraping terminé (ou en cas d'erreur), le driver est fermé.\n",
    "\n",
    "La fonction retourne les items scrapés ainsi que l'ensemble des clés de spécifications pour la sauvegarde.\n"
   ],
   "id": "11a6335fe7d29de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def scrape_ubuy(driver, base_url, max_pages):\n",
    "    \"\"\"Scrape les données produit sur plusieurs pages d'Ubuy.\"\"\"\n",
    "    scraped_items = []\n",
    "    all_spec_keys = set()\n",
    "\n",
    "    try:\n",
    "        current_page = 1\n",
    "        current_url = base_url\n",
    "\n",
    "        while current_page <= max_pages:\n",
    "            logging.info(f\"Scraping page {current_page}: {current_url}\")\n",
    "            driver.get(current_url)\n",
    "            time.sleep(random.uniform(3, 6))  # Délai aléatoire\n",
    "\n",
    "            # Vérifie la présence d'un CAPTCHA\n",
    "            try:\n",
    "                WebDriverWait(driver, 5).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, \"iframe[src*='captcha']\"))\n",
    "                )\n",
    "                handle_captcha(driver)\n",
    "            except:\n",
    "                logging.info(\"No CAPTCHA detected. Proceeding with scraping...\")\n",
    "\n",
    "            # Attendre que les produits soient chargés\n",
    "            try:\n",
    "                WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, \"div.product-card\"))\n",
    "                )\n",
    "            except Exception as e:\n",
    "                logging.error(\"No products found. Page may have changed.\")\n",
    "                break\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            product_blocks = soup.find_all('div', class_='product-card')\n",
    "\n",
    "            if not product_blocks:\n",
    "                logging.info(\"No products found. Exiting scraping.\")\n",
    "                break\n",
    "\n",
    "            # Récupérer les URLs des produits\n",
    "            product_urls = []\n",
    "            for product in product_blocks:\n",
    "                link_element = product.find('a', class_='product-img')\n",
    "                if link_element and \"href\" in link_element.attrs:\n",
    "                    product_url = link_element['href']\n",
    "                    full_product_url = f\"https://www.ubuy.ma{product_url}\" if product_url.startswith('/') else product_url\n",
    "                    product_urls.append(full_product_url)\n",
    "\n",
    "            # Scraping concurrent des détails produits\n",
    "            with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "                future_to_url = {executor.submit(scrape_product_details, driver, url): url for url in product_urls}\n",
    "                for future in as_completed(future_to_url):\n",
    "                    url = future_to_url[future]\n",
    "                    try:\n",
    "                        specifications = future.result()\n",
    "                        all_spec_keys.update(specifications.keys())\n",
    "\n",
    "                        # Rechercher le bloc produit correspondant pour récupérer title, price, image_url\n",
    "                        for product in product_blocks:\n",
    "                            link_element = product.find('a', class_='product-img')\n",
    "                            if link_element and \"href\" in link_element.attrs:\n",
    "                                product_url = link_element['href']\n",
    "                                full_product_url = f\"https://www.ubuy.ma{product_url}\" if product_url.startswith('/') else product_url\n",
    "                                if full_product_url == url:\n",
    "                                    title = product.find('h3', class_='product-title').text.strip() if product.find('h3', class_='product-title') else \"No title\"\n",
    "                                    price = product.find('p', class_='product-price').text.strip() if product.find('p', class_='product-price') else \"No price\"\n",
    "                                    image_url = product.find('img')['src'] if product.find('img') else \"No image\"\n",
    "\n",
    "                                    scraped_items.append({\n",
    "                                        \"title\": title,\n",
    "                                        \"price\": price,\n",
    "                                        \"image_url\": image_url,\n",
    "                                        \"product_url\": full_product_url,\n",
    "                                        \"specifications\": specifications\n",
    "                                    })\n",
    "                                    break\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error processing {url}: {e}\")\n",
    "\n",
    "            # Recherche du lien vers la page suivante\n",
    "            next_page_element = soup.find('li', class_='page-item', title=str(current_page + 1))\n",
    "            if next_page_element:\n",
    "                next_button = next_page_element.find('button', class_='page-link')\n",
    "                if next_button and \"data-pageno\" in next_button.attrs:\n",
    "                    next_page_number = next_button['data-pageno']\n",
    "                    current_url = f\"{base_url}&page={next_page_number}\"\n",
    "                    current_page += 1\n",
    "                    time.sleep(random.uniform(3, 7))\n",
    "                else:\n",
    "                    logging.info(\"No more pages found.\")\n",
    "                    break\n",
    "            else:\n",
    "                logging.info(\"No more pages found.\")\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during scraping: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    return scraped_items, all_spec_keys\n"
   ],
   "id": "a0f5849e6042f824"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Bloc Principal d'Exécution\n",
    "\n",
    "Dans ce bloc, nous définissons un dictionnaire `categories` qui associe à chaque catégorie :\n",
    "- Une URL de base pour démarrer le scraping.\n",
    "- Le nombre maximum de pages à parcourir.\n",
    "\n",
    "Pour chaque catégorie :\n",
    "1. Nous initialisons le driver avec `get_driver()`.\n",
    "2. Nous appelons `scrape_ubuy` pour récupérer les données produits.\n",
    "3. Si des données ont été récupérées, nous les sauvegardons dans un fichier CSV à l'aide de `save_to_csv`.\n",
    "\n",
    "En cas d'erreur, le message sera loggé.\n"
   ],
   "id": "92ad60016c65981c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        logging.info(\"Starting script...\")\n",
    "        categories = {\n",
    "            \"graphics_cards\": (\"https://www.ubuy.ma/en/search/?ref_p=ser_tp&q=graphics+cards\", 8),\n",
    "            \"laptops\": (\"https://www.ubuy.ma/en/category/laptops-21457\", 8),\n",
    "            \"monitors\": (\"https://www.ubuy.ma/en/search/?q=computer%20monitor\", 8),\n",
    "            \"smart_watches\": (\"https://www.ubuy.ma/en/search/?ref_p=ser_tp&q=smart+watch\", 8)\n",
    "        }\n",
    "\n",
    "        for category, (base_url, max_pages) in categories.items():\n",
    "            logging.info(f\"Scraping {category}...\")\n",
    "            driver = get_driver()\n",
    "            scraped_data, all_spec_keys = scrape_ubuy(driver, base_url, max_pages)\n",
    "\n",
    "            if scraped_data:\n",
    "                save_to_csv(scraped_data, category, all_spec_keys)\n",
    "            else:\n",
    "                logging.info(f\"No data scraped for {category}.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred: {e}\")\n"
   ],
   "id": "3e199a91f1eff859"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "Ce notebook démontre comment utiliser Selenium et Undetected Chromedriver pour scraper des informations détaillées depuis Ubuy.\n",
    "Chaque fonction a été commentée en détail pour expliquer son rôle et son fonctionnement :\n",
    "\n",
    "- **get_random_user_agent** : Retourne un User-Agent aléatoire.\n",
    "- **get_driver** : Initialise et configure le driver pour contourner la détection.\n",
    "- **handle_captcha** : Permet à l'utilisateur de résoudre manuellement un CAPTCHA.\n",
    "- **scrape_product_details** : Extrait les spécifications d'un produit depuis sa page.\n",
    "- **get_next_scrape_number** : Gère la version des fichiers de sortie.\n",
    "- **save_to_csv** : Enregistre les données dans un fichier CSV.\n",
    "- **scrape_ubuy** : Parcourt plusieurs pages d'une catégorie et scrape les produits.\n",
    "\n",
    "Avant d'exécuter ce notebook, assurez-vous d'avoir installé les dépendances requises (par exemple, via `pip install selenium undetected-chromedriver webdriver-manager bs4`).\n",
    "\n",
    "Bonne utilisation et bon scraping !\n"
   ],
   "id": "aab3b4f3e83c4fd2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
